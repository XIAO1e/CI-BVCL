"""
å›ºå®šç»´åº¦ç‰ˆæœ¬çš„ EEG Backbone
- åœ¨åˆå§‹åŒ–æ—¶å°±è®¡ç®—å¹¶åˆ›å»ºå®Œæ•´ç½‘ç»œï¼ˆä¸å»¶è¿Ÿï¼‰
- è‡ªåŠ¨é€‚é… EEG å’Œ MEG æ•°æ®é›†
- ä¸éœ€è¦é¢„åˆå§‹åŒ–
- è®­ç»ƒç¨³å®š
"""

import torch.nn as nn
from einops.layers.torch import Rearrange
from torch import Tensor
import os
import logging
from torch.utils.data import Dataset, DataLoader
import numpy as np
import torch


class ResidualAdd(nn.Module):
    def __init__(self, f):
        super().__init__()
        self.f = f

    def forward(self, x):
        return  x + self.f(x)
    

class EEGProjectLayer(nn.Module):
    def __init__(self,  z_dim,c_num, timesteps, drop_proj=0.3):
        super(EEGProjectLayer, self).__init__()
        self.z_dim = z_dim
        self.c_num = c_num
        self.timesteps = timesteps

        self.input_dim = self.c_num * (self.timesteps[1]-self.timesteps[0])
        proj_dim = z_dim

        self.model = nn.Sequential(nn.Linear(self.input_dim, proj_dim),
            ResidualAdd(nn.Sequential(
                nn.GELU(),
                nn.Linear(proj_dim, proj_dim),
                nn.Dropout(drop_proj),
            )),
            nn.LayerNorm(proj_dim))
        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))
        self.softplus = nn.Softplus()
        
    def forward(self, x):
        x = x.view(x.shape[0], self.input_dim)
        x = self.model(x)
        return x


class FlattenHead(nn.Sequential):
    def __init__(self):
        super().__init__()

    def forward(self, x):
        x = x.contiguous().view(x.size(0), -1)
        return x


def _calculate_conv_output_dim(c_num, timesteps):
    """
    ä½¿ç”¨å®é™…æµ‹è¯•æ¥ç¡®å®šå„ä¸ª backbone çš„è¾“å‡ºç»´åº¦
    è¿™æ¯”æ‰‹åŠ¨è®¡ç®—æ›´å¯é 
    
    Args:
        c_num: é€šé“æ•°
        timesteps: [start, end] æ—¶é—´çª—
    
    Returns:
        å„ä¸ª backbone çš„ embedding_dim
    """
    t_len = timesteps[1] - timesteps[0]
    
    # ä½¿ç”¨å·²çŸ¥çš„ç»´åº¦è¡¨ï¼ˆä»å®é™…æµ‹è¯•å¾—å‡ºï¼‰
    # EEG: c_num=17, t_len=250
    # MEG: c_num=271, t_len=201
    
    if c_num == 17 and t_len == 250:
        # EEG é…ç½®
        return {
            'shallownet': 1440,
            'tsconv': 1440,
            'deepnet': 1400,
            'eegnet': 1248,
        }
    elif c_num == 271 and t_len == 201:
        # MEG é…ç½®ï¼ˆä»ä¹‹å‰æµ‹è¯•å¾—å‡ºï¼‰
        return {
            'shallownet': 1040,
            'tsconv': 1040,
            'deepnet': 800,
            'eegnet': 864,
        }
    else:
        # å…¶ä»–é…ç½®ï¼šåŠ¨æ€è®¡ç®—ï¼ˆä½¿ç”¨dummy forwardï¼‰
        # è¿”å› Noneï¼Œåœ¨ BaseModel ä¸­ä¼šè§¦å‘åŠ¨æ€è®¡ç®—
        return {
            'shallownet': None,
            'tsconv': None,
            'deepnet': None,
            'eegnet': None,
        }


class BaseModel(nn.Module):
    """
    å›ºå®šç»´åº¦ç‰ˆæœ¬çš„ BaseModel
    - å¯¹äºå·²çŸ¥é…ç½®ï¼ˆEEG/MEGï¼‰ï¼Œåœ¨ __init__ æ—¶å°±åˆ›å»ºå®Œæ•´çš„ project å±‚
    - å¯¹äºæœªçŸ¥é…ç½®ï¼Œä½¿ç”¨ lazy initializationï¼ˆå‘åå…¼å®¹ï¼‰
    - ä¸éœ€è¦é¢„åˆå§‹åŒ–
    """
    def __init__(self, z_dim, c_num, timesteps, backbone_type='shallownet'):
        super(BaseModel, self).__init__()
        
        self.z_dim = z_dim
        self.c_num = c_num
        self.timesteps = timesteps
        self.backbone_type = backbone_type.lower()
        
        # è®¡ç®—è¯¥é…ç½®ä¸‹çš„æ­£ç¡® embedding_dim
        dims = _calculate_conv_output_dim(c_num, timesteps)
        embedding_dim = dims.get(self.backbone_type, None)
        
        self.backbone = None  # å­ç±»ä¼šè®¾ç½®
        
        if embedding_dim is not None:
            # å·²çŸ¥é…ç½®ï¼šç«‹å³åˆ›å»º project å±‚
            self.project = nn.Sequential(
                FlattenHead(),
                nn.Linear(embedding_dim, z_dim),
                ResidualAdd(nn.Sequential(
                    nn.GELU(),
                    nn.Linear(z_dim, z_dim),
                    nn.Dropout(0.5))),
                nn.LayerNorm(z_dim))
            print(f"[{self.backbone_type.upper()}] c_num={c_num}, timesteps={timesteps}, embedding_dim={embedding_dim} âœ…")
        else:
            # æœªçŸ¥é…ç½®ï¼šå»¶è¿Ÿåˆ›å»ºï¼ˆä½†ä¼šæ‰“å°è­¦å‘Šï¼‰
            self.project = None
            print(f"[{self.backbone_type.upper()}] c_num={c_num}, timesteps={timesteps}, âš ï¸  Unknown config, will use lazy init")
        
        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))
        self.softplus = nn.Softplus()
    
    def _build_projector(self, in_features: int):
        """ä»…ç”¨äºæœªçŸ¥é…ç½®çš„å»¶è¿Ÿåˆå§‹åŒ–"""
        self.project = nn.Sequential(
            FlattenHead(),
            nn.Linear(in_features, self.z_dim),
            ResidualAdd(nn.Sequential(
                nn.GELU(),
                nn.Linear(self.z_dim, self.z_dim),
                nn.Dropout(0.5))),
            nn.LayerNorm(self.z_dim))
        # ç¡®ä¿åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
        if self.backbone is not None:
            self.project.to(next(self.backbone.parameters()).device)

    def forward(self,x):
        x = x.unsqueeze(1)
        x = self.backbone(x)
        
        # å¦‚æœæ˜¯æœªçŸ¥é…ç½®ï¼Œç¬¬ä¸€æ¬¡ forward æ—¶åˆ›å»º
        if self.project is None:
            with torch.no_grad():
                flat_dim = x.contiguous().view(x.size(0), -1).size(1)
            self._build_projector(flat_dim)
            print(f"[{self.backbone_type.upper()}] Lazy init with embedding_dim={flat_dim}")
        
        x = self.project(x)
        return x


class Shallownet(BaseModel):
    def __init__(self, z_dim, c_num, timesteps):
        super().__init__(z_dim, c_num, timesteps, backbone_type='shallownet')
        self.backbone = nn.Sequential(
                nn.Conv2d(1, 40, (1, 25), (1, 1)),
                nn.Conv2d(40, 40, (c_num, 1), (1, 1)),
                nn.BatchNorm2d(40),
                nn.ELU(),
                nn.AvgPool2d((1, 51), (1, 5)),
                nn.Dropout(0.5),
            )
    

class Deepnet(BaseModel):
    def __init__(self, z_dim, c_num, timesteps):
        super().__init__(z_dim, c_num, timesteps, backbone_type='deepnet')
        self.backbone = nn.Sequential(
                nn.Conv2d(1, 25, (1, 10), (1, 1)),
                nn.Conv2d(25, 25, (c_num, 1), (1, 1)),
                nn.BatchNorm2d(25),
                nn.ELU(),
                nn.MaxPool2d((1, 2), (1, 2)),
                nn.Dropout(0.5),

                nn.Conv2d(25, 50, (1, 10), (1, 1)),
                nn.BatchNorm2d(50),
                nn.ELU(),
                nn.MaxPool2d((1, 2), (1, 2)),
                nn.Dropout(0.5),

                nn.Conv2d(50, 100, (1, 10), (1, 1)),
                nn.BatchNorm2d(100),
                nn.ELU(),
                nn.MaxPool2d((1, 2), (1, 2)),
                nn.Dropout(0.5),

                nn.Conv2d(100, 200, (1, 10), (1, 1)),
                nn.BatchNorm2d(200),
                nn.ELU(),
                nn.MaxPool2d((1, 2), (1, 2)),
                nn.Dropout(0.5),
            )
        

class EEGnet(BaseModel):
    def __init__(self,  z_dim, c_num, timesteps):
        super().__init__(z_dim, c_num, timesteps, backbone_type='eegnet')
        self.backbone = nn.Sequential(
                nn.Conv2d(1, 8, (1, 64), (1, 1)),
                nn.BatchNorm2d(8),
                nn.Conv2d(8, 16, (c_num, 1), (1, 1)),
                nn.BatchNorm2d(16),
                nn.ELU(),
                nn.AvgPool2d((1, 2), (1, 2)),
                nn.Dropout(0.5),
                nn.Conv2d(16, 16, (1, 16), (1, 1)),
                nn.BatchNorm2d(16), 
                nn.ELU(),
                nn.Dropout2d(0.5)
            )
        

class TSconv(BaseModel):
    def __init__(self, z_dim, c_num, timesteps):
        super().__init__(z_dim, c_num, timesteps, backbone_type='tsconv')
        self.backbone = nn.Sequential(
                nn.Conv2d(1, 40, (1, 25), (1, 1)),
                nn.AvgPool2d((1, 51), (1, 5)),
                nn.BatchNorm2d(40),
                nn.ELU(),
                nn.Conv2d(40, 40, (c_num, 1), (1, 1)),
                nn.BatchNorm2d(40),
                nn.ELU(),
                nn.Dropout(0.5),
            )


if __name__ == "__main__":
    print("="*80)
    print("æµ‹è¯•å›ºå®šç»´åº¦ç‰ˆæœ¬ - EEG é…ç½®")
    print("="*80)
    
    backbones = {
        "Shallownet": Shallownet,
        "Deepnet": Deepnet,
        "EEGnet": EEGnet,
        "TSconv": TSconv,
    }
    
    # EEG é…ç½®
    z_dim = 1024
    c_num_eeg = 17
    timesteps_eeg = [0, 250]
    
    print("\nğŸ“Š EEG æ•°æ®é›† (c_num=17, timesteps=250):")
    print("-"*80)
    for name, BackboneClass in backbones.items():
        model = BackboneClass(z_dim=z_dim, c_num=c_num_eeg, timesteps=timesteps_eeg)
        total_params = sum(p.numel() for p in model.parameters())
        
        # æµ‹è¯•å‰å‘ä¼ æ’­
        with torch.no_grad():
            x = torch.randn(2, c_num_eeg, timesteps_eeg[1]-timesteps_eeg[0])
            out = model(x)
        
        print(f"  {name:<15} å‚æ•°: {total_params:>10,}  è¾“å‡º: {out.shape}")
    
    # MEG é…ç½®
    c_num_meg = 271
    timesteps_meg = [0, 201]
    
    print("\nğŸ“Š MEG æ•°æ®é›† (c_num=271, timesteps=201):")
    print("-"*80)
    for name, BackboneClass in backbones.items():
        model = BackboneClass(z_dim=z_dim, c_num=c_num_meg, timesteps=timesteps_meg)
        total_params = sum(p.numel() for p in model.parameters())
        
        # æµ‹è¯•å‰å‘ä¼ æ’­
        with torch.no_grad():
            x = torch.randn(2, c_num_meg, timesteps_meg[1]-timesteps_meg[0])
            out = model(x)
        
        print(f"  {name:<15} å‚æ•°: {total_params:>10,}  è¾“å‡º: {out.shape}")
    
    print("\n" + "="*80)
    print("âœ… å›ºå®šç»´åº¦ç‰ˆæœ¬æµ‹è¯•é€šè¿‡ï¼")
    print("   - åœ¨åˆå§‹åŒ–æ—¶å°±åˆ›å»ºå®Œæ•´ç½‘ç»œ")
    print("   - ä¸éœ€è¦é¢„åˆå§‹åŒ–")
    print("   - åŒæ—¶æ”¯æŒ EEG å’Œ MEG")
    print("="*80)

